import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_chroma import Chroma
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ç¢ºä¿è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

class InsuranceRagEngine:
    def __init__(self, persist_dir: str = None):
        """
        åˆå§‹åŒ– RAG å¼•æ“
        :param persist_dir: æŒ‡å®šè³‡æ–™åº«è·¯å¾‘ã€‚è‹¥ç‚º Noneï¼Œå‰‡å˜—è©¦å¾ .envè®€å–ï¼Œé è¨­ç‚º ./chroma_db
        """
        self.api_key = os.getenv("gemini_key")
        if not self.api_key:
            raise ValueError("âŒ éŒ¯èª¤: ç’°å¢ƒè®Šæ•¸ 'gemini_key' æœªè¨­å®š")

        # 1. æ±ºå®šè³‡æ–™åº«è·¯å¾‘ (ä¿®å¾©è·¯å¾‘éŒ¯äº‚å•é¡Œçš„æ ¸å¿ƒ)
        if persist_dir is None:
            self.persist_dir = os.getenv("persist_dir", "./chroma_db")
        else:
            self.persist_dir = persist_dir

        print(f"ğŸ”§ [Init] æ­£åœ¨è¼‰å…¥å‘é‡è³‡æ–™åº«ï¼Œè·¯å¾‘: {os.path.abspath(self.persist_dir)}")

        # 2. åˆå§‹åŒ– Embedding (å¿…é ˆèˆ‡å»ºåº«æ™‚ä½¿ç”¨çš„æ¨¡å‹ä¸€è‡´)
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004", # ç¢ºä¿ä½¿ç”¨ 004
            google_api_key=self.api_key
        )
        
        # 3. åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
        # æ³¨æ„: é€™è£¡ä¸æœƒæª¢æŸ¥è·¯å¾‘æ˜¯å¦å­˜åœ¨ï¼Œè‹¥è·¯å¾‘éŒ¯äº†æœƒå»ºç«‹ä¸€å€‹ç©ºçš„ DB
        self.vectorstore = Chroma(
            collection_name="Travel_Insurance_RAG",
            persist_directory=self.persist_dir,
            embedding_function=self.embeddings
        )

        # æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦çœŸçš„æœ‰è³‡æ–™ (è‡ªæˆ‘è¨ºæ–·)
        count = self.vectorstore._collection.count()
        if count == 0:
            print(f"âš ï¸ è­¦å‘Š: è¼‰å…¥çš„è³‡æ–™åº« '{self.persist_dir}' æ˜¯ç©ºçš„ï¼è«‹æª¢æŸ¥è·¯å¾‘æˆ–é‡æ–°å»ºåº«ã€‚")
        else:
            print(f"âœ… è³‡æ–™åº«è¼‰å…¥æˆåŠŸï¼Œç›®å‰å…±æœ‰ {count} ç­†è³‡æ–™ã€‚")
        
        # 4. åˆå§‹åŒ– LLM
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-2.5-flash",
            google_api_key=self.api_key,
            temperature=0
        )
        
        self.setup_prompts()

    def setup_prompts(self):
        # 1. æŸ¥è©¢ç”Ÿæˆ Prompt
        self.query_gen_template = """
        ä½ æ˜¯ä¸€å€‹ç²¾é€šä¿éšªæ¢æ¬¾çš„å°ˆæ¥­åˆ†æå¸«ã€‚ä½¿ç”¨è€…çš„å•é¡Œå¯èƒ½åŒ…å«å£èªæˆ–æ¨¡ç³Šæ¦‚å¿µã€‚
        è«‹æ ¹æ“šä½¿ç”¨è€…çš„å•é¡Œï¼Œç”¢ç”Ÿ 4 å€‹å…·å‚™ã€Œé«˜èªæ„å·®ç•°ã€çš„æœå°‹æŸ¥è©¢ï¼Œä»¥ç¢ºä¿æª¢ç´¢èƒ½è¦†è“‹æ‰€æœ‰æ½›åœ¨æ¢æ¬¾ã€‚

        ç”¢ç”Ÿè¦å‰‡ï¼š
        1. **è¡“èªç²¾ç¢ºåŒ–**ï¼šå°‡å£èªè½‰æ›ç‚ºæ¨™æº–è¡“èªï¼ˆå¦‚ã€Œé£›æ©Ÿé²åˆ°ã€è½‰ç‚ºã€Œç­æ©Ÿå»¶èª¤ã€ã€ã€Œæ—…ç¨‹å»¶èª¤ã€ï¼‰ã€‚
        2. **æ¦‚å¿µè¾¨æ (æ ¸å¿ƒ)**ï¼šè‹¥å•é¡Œæ¶‰åŠã€Œå»¶èª¤ã€æˆ–ã€Œæå¤±ã€ï¼Œè«‹åˆ†åˆ¥é‡å°ã€Œæ—…ç¨‹/ç­æ©Ÿã€èˆ‡ã€Œè¡Œæã€ç”¢ç”Ÿç¨ç«‹æŸ¥è©¢ï¼Œé¿å…å…©è€…æ··æ·†ã€‚
           - ç¯„ä¾‹ï¼šè‹¥å•å»¶èª¤ï¼Œæ‡‰åŒæ™‚æœå°‹ã€Œæ—…ç¨‹å»¶èª¤æ‰¿ä¿ç¯„åœã€èˆ‡ã€Œç­æ©Ÿå»¶èª¤çµ¦ä»˜ã€ã€‚
        3. **ç¶­åº¦æ“´å±•**ï¼šæŸ¥è©¢æ‡‰æ¶µè“‹ã€Œæ‰¿ä¿äº‹æ•…ã€ã€ã€Œç†è³ æ¨™æº–ã€ã€ã€Œç†è³ æ–‡ä»¶ã€èˆ‡ã€Œé™¤å¤–è²¬ä»»ã€å››å€‹ç¶­åº¦ã€‚
        4. **æ’é™¤å¹²æ“¾**ï¼šè‹¥ä½¿ç”¨è€…æœªæåŠã€Œè¡Œæã€ï¼ŒæŸ¥è©¢æ‡‰å´é‡æ–¼ã€Œæ—…ç¨‹ã€èˆ‡ã€Œäº¤é€šå·¥å…·ã€æœ¬èº«ã€‚

        ä½¿ç”¨è€…å•é¡Œï¼š{question}

        è«‹ç›´æ¥åˆ—å‡º 4 å€‹æŸ¥è©¢ï¼Œæ¯è¡Œä¸€å€‹ï¼Œä¸è¦æœ‰ç·¨è™Ÿæˆ–é¡å¤–æ–‡å­—ï¼š
        """
        self.query_gen_prompt = PromptTemplate(
            template=self.query_gen_template,
            input_variables=["question"]
        )

        # 2. å›ç­”ç”Ÿæˆ Prompt
        self.qa_template = """
        ä½ æ˜¯ä¸€ä½è³‡æ·±çš„ä¿éšªç†è³ é¡§å•ã€‚è«‹æ ¹æ“šä¿éšªæ¢æ¬¾å…§å®¹å›ç­”ä½¿ç”¨è€…çš„ç†è³ å•é¡Œã€‚
        
        ã€æ¢æ¬¾å…§å®¹ã€‘:
        {context}
        
        ä½¿ç”¨è€…å•é¡Œï¼š{question}
        
        å›ç­”è¦ç¯„ï¼ˆè«‹åš´æ ¼éµå®ˆï¼‰ï¼š
        1. **è‡ªç„¶å°è©±**ï¼šè«‹ç›´æ¥é‡å°å•é¡Œçµ¦å‡ºå»ºè­°ï¼Œç¦æ­¢ä½¿ç”¨ã€Œæ ¹æ“šæ‚¨æä¾›çš„ Contextã€ã€ã€Œæ ¹æ“šåƒè€ƒè³‡æ–™ã€ã€ã€Œæ ¹æ“šæª¢ç´¢åˆ°çš„å…§å®¹ã€æˆ–ã€Œåœ¨æœ¬æ¬¡æä¾›çš„æ–‡ä»¶ä¸­ã€ç­‰é¡ä¼¼è¡“èªã€‚
        2. **å°ˆæ¥­ç«‹å ´**ï¼šä½ çš„å›ç­”æ‡‰åƒæ˜¯åœ¨é¢å°é¢è«®è©¢å®¢æˆ¶ï¼Œèªæ°£è¦å°ˆæ¥­ä¸”èª æ‡‡ï¼Œç›´æ¥å¼•ç”¨æ¢æ¬¾åç¨±èˆ‡æ¢è™Ÿï¼ˆä¾‹å¦‚ï¼šæ ¹æ“šç¬¬ XX æ¢è¦å®š...ï¼‰ã€‚
        3. **å®Œæ•´æ€§**ï¼šè«‹ç¶œåˆè³‡è¨Šï¼Œä¸è¦éºæ¼é‡è¦æ¢æ¬¾ï¼Œç‰¹åˆ¥æ˜¯ã€Œç‰¹åˆ¥ä¸ä¿äº‹é …ã€æˆ–ã€Œé™¤å¤–è²¬ä»»ã€ã€‚
        4. **èª å¯¦è™•ç†**ï¼šå¦‚æœæ¢æ¬¾ä¸­ç¢ºå¯¦æ²’æœ‰æåˆ°ç›¸é—œè³‡è¨Šï¼Œè«‹å§”å©‰å‘ŠçŸ¥ç›®å‰æ¢æ¬¾å…§å®¹æœªæ¶µè“‹æ­¤é …ç›®ï¼Œä¸è¦ç·¨é€ ç­”æ¡ˆã€‚
        
        è«‹ç›´æ¥é–‹å§‹å›ç­”ï¼š
        """
        self.qa_prompt = PromptTemplate(
            template=self.qa_template,
            input_variables=["context", "question"]
        )

    def generate_search_queries(self, question: str) -> list[str]:
        """åˆ©ç”¨ LLM ç”¢ç”Ÿå¤šæ¨£åŒ–çš„æœå°‹å­—ä¸²"""
        chain = self.query_gen_prompt | self.llm | StrOutputParser()
        result = chain.invoke({"question": question})
        queries = [q.strip() for q in result.split('\n') if q.strip()]
        return queries[:5]

    def get_answer(self, user_question: str, chat_history_list: list = []):
        # --- æ­¥é©Ÿ 1: ç”Ÿæˆå¤šé‡æŸ¥è©¢ ---
        generated_queries = self.generate_search_queries(user_question)
        if user_question not in generated_queries:
            generated_queries.insert(0, user_question)

        print(f"\n[DEBUG] åŸå§‹å•é¡Œ: {user_question}")
        print(f"\n[DEBUG] åŸ·è¡Œæœå°‹ç­–ç•¥: {generated_queries}")
        # --- æ­¥é©Ÿ 2: åŸ·è¡Œå¤šé‡æª¢ç´¢ (MMR) ---
        
        retriever = self.vectorstore.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": 6, 
                "fetch_k": 30, 
                "lambda_mult": 0.5
            }
        )
        
        # ä½¿ç”¨å­—å…¸ä¾†å„²å­˜ï¼Œé¿å…ç°¡å–® extend å°è‡´çš„é †åºåå·®
        query_results = []
        for query in generated_queries:
            docs = retriever.invoke(query)
            query_results.append(docs)

        # --- æ­¥é©Ÿ 3: æ™ºæ…§äº¤éŒ¯æ’åº (Interleaving) ---
        # ç¢ºä¿æ¯å€‹æœå°‹é—œéµå­—æŠ“åˆ°çš„ç¬¬ä¸€åéƒ½æœ‰æ©Ÿæœƒé€²å…¥æœ€çµ‚ Context
        all_docs = []
        max_docs_per_query = 8
        for i in range(max_docs_per_query):
            for docs in query_results:
                if i < len(docs):
                    all_docs.append(docs[i])

        # --- æ­¥é©Ÿ 4: å»é™¤é‡è¤‡æ–‡ä»¶ ---
        unique_docs = []
        seen_content = set()
        
        for doc in all_docs:
            content_snippet = doc.page_content.strip()
            # ä½¿ç”¨å…¨æ–‡é›œæ¹Šæˆ–æ¢è™Ÿä½œç‚ºå»é‡æ¨™æº–
            doc_id = doc.metadata.get('section_id', doc.metadata.get('article_no', content_snippet[:100]))
            
            if content_snippet not in seen_content:
                unique_docs.append(doc)
                seen_content.add(content_snippet)
        
        # æ“´å¤§ Context è¦–çª— (Gemini 2.5 Flash è™•ç†èƒ½åŠ›å¼·ï¼Œå¯ä»¥å¤šçµ¦ä¸€é»è³‡æ–™)
        final_docs = unique_docs[:18] 
        
        print(f"[DEBUG] æœ€çµ‚åƒèˆ‡å›ç­”çš„æ¢æ¬¾ç‰‡æ®µæ•¸: {len(final_docs)}")
        # é™¤éŒ¯ï¼šå°å‡ºç›®å‰æŠ“åˆ°çš„æ¢è™Ÿï¼Œç¢ºèªã€Œç­æ©Ÿå»¶èª¤ã€æœ‰æ²’æœ‰é€²ä¾†
        retrieved_articles = [d.metadata.get('article_no', 'Unknown') for d in final_docs]
        print(f"[DEBUG] æª¢ç´¢åˆ°çš„æ¢è™Ÿæ¸…å–®: {retrieved_articles}")

        # --- æ­¥é©Ÿ 5: çµ„è£ Context ---
        def format_doc(doc):
            meta = doc.metadata
            source = meta.get('source', 'ä¿éšªæ¢æ¬¾')
            sec_id = meta.get('section_id', meta.get('article_no', 'ç„¡æ¢è™Ÿ'))
            title = meta.get('title', meta.get('article_title', 'ç„¡æ¨™é¡Œ'))
            return f"ğŸ“„ ä¾†æºï¼š{source} | âš–ï¸ æ¢è™Ÿï¼š{sec_id} | ğŸ“ æ¨™é¡Œï¼š{title}\nğŸ“– å…§å®¹ï¼š{doc.page_content}"

        context_text = "\n--------------------\n".join([format_doc(d) for d in final_docs])

        if not context_text:
            return {"answer": "ç›®å‰çš„æ¢æ¬¾è³‡æ–™ä¸­æœªæŸ¥ç²ç›¸é—œå…§å®¹...", "source_documents": [], "debug_queries": generated_queries}

        # --- æ­¥é©Ÿ 6: ç”Ÿæˆå›ç­” ---
        chain = self.qa_prompt | self.llm | StrOutputParser()
        response = chain.invoke({"context": context_text, "question": user_question})
        
        return {"answer": response, "source_documents": final_docs, "debug_queries": generated_queries}